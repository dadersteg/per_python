"""
============================================================================
 NIAP Master Spine Reconciliation v1.0.0
============================================================================

EXECUTIVE SUMMARY (BLUF)
This engine creates a bi-directional "Master Spine" that reconciles every
NIAP Governance Ticket against every Product Register entry. It identifies
gaps in both directions: tickets without products and products without tickets.

Main Benefit: Provides Afonso and POs with a single, filterable "Venn Diagram"
table for forensic investigation into "Approved for Launch" gaps and "Shadow IT."

Business Logic: Reconciles governance tickets against the product register
to ensure all launched products have proper NIAP governance.
"""

import pandas as pd
import re
import time
import traceback
import logging
from datetime import datetime
from rapidfuzz import utils

# -----------------------------------------------------------------------------
# 1. SETUP: Configuration and File Naming
# -----------------------------------------------------------------------------
TARGET_FOLDER_ID = "15xKraT4MhWzBvOjMTJ0x21TBIopqfl4E"
TIMESTAMP = datetime.now().strftime("%Y%m%d_%H%M")

# Core Artifacts
REPORT_MD = f"niap_forensic_deep_dive_{TIMESTAMP}.md"
CSV_TOC = f"export_table_of_contents_{TIMESTAMP}.csv"
CSV_MASTER_SPINE = f"niap_master_spine_reconciliation_{TIMESTAMP}.csv"

# Supporting Bases
CSV_BASE_0 = f"base_0_register_delta_master_{TIMESTAMP}.csv"
CSV_BASE_1_GAPS = f"base_1_core_GAPS_RISK_priority_{TIMESTAMP}.csv"
CSV_BASE_2 = f"base_2_component_mapping_{TIMESTAMP}.csv"

# Audit Trail
TXT_METADATA = f"audit_provenance_metadata_{TIMESTAMP}.txt"

# -----------------------------------------------------------------------------
# 2. TOOLS: Data Processing & Formatting
# -----------------------------------------------------------------------------

def df_to_markdown_manual(df, headers=None):
    """Formats a DataFrame into a clean Markdown table.

    Why: To provide a human-readable table format in the forensic report without
    encountering common f-string backslash errors.

    Args:
        df (pd.DataFrame or pd.Series): The data to format into a table.
        headers (list, optional): Custom headers for the table. Defaults to None.

    Returns:
        str: A Markdown formatted string representing the table.
    """
    if df.empty: return "No data available."
    
    if isinstance(df, pd.Series):
        df = df.reset_index()
        df.columns = headers if headers else ["Category", "Count"]
    
    if isinstance(df.index, pd.Index) and df.index.name:
        df = df.reset_index()

    if headers and len(headers) == len(df.columns):
        df.columns = headers
    
    md_headers = df.columns.tolist()
    header_row = f"| {' | '.join(md_headers)} |"
    sep_row = f"| {' | '.join(['---'] * len(md_headers))} |"
    
    body = []
    for _, row in df.iterrows():
        clean_values = []
        for val in row:
            s_val = str(val).replace('|', '\\|').replace('\n', ' ')
            clean_values.append(s_val)
        row_str = "| " + " | ".join(clean_values) + " |"
        body.append(row_str)
        
    return f"{header_row}\n{sep_row}\n" + "\n".join(body)

def normalize_name(text):
    """Cleans and standardizes names for cross-source matching.

    Why: To ensure consistent matching between different data sources (e.g., Jira
    vs Product Register) by removing noise and standardizing casing.

    Args:
        text (str): The text to normalize.

    Returns:
        str: The normalized text.
    """
    if pd.isna(text): return ""
    text = str(text).lower()
    text = re.split(r'[|Â·]', text)[-1].strip() 
    return utils.default_process(text)

def clean_tech_family(text):
    """Aggregates technical configurations into clean product families.

    Why: To simplify complex technical names by removing environment-specific
    suffixes and common product prefixes, allowing for better aggregation.

    Args:
        text (str): The technical name to clean.

    Returns:
        str: The cleaned product family name.
    """
    if pd.isna(text): return ""
    text = str(text).upper()
    text = re.sub(r'_(GB|EUR|US|AU|NZ|SG|JP|RO|FR|PL|ES|IT|IE)_?', ' ', text)
    for p in [r'PT_', r'MF_', r'ACQ_', r'REVX_', r'RV_', r'PRCHS_']: text = re.sub(p, ' ', text)
    for n in ['REPRICING','PLAN','BASE','STD','PREM','METAL','PLUS','OFFER','FREE','UL','PR','ST']: text = text.replace(n, '')
    return " ".join(re.sub(r'[^A-Z0-9\s]', ' ', text).split()).lower()

def upload_to_drive(file_path, mime_type):
    """Uploads a file to Google Drive with retries.

    Why: To ensure the generated forensic reports and data files are safely
    archived in a shared location for stakeholder review.

    Args:
        file_path (str): The path to the file to upload.
        mime_type (str): The MIME type of the file.
    """
    for attempt in range(3):
        try:
            upload_file_to_gdrive(file=file_path, name=file_path, mime_type=mime_type, folder_id=TARGET_FOLDER_ID)
            return
        except Exception: time.sleep(5)

# -----------------------------------------------------------------------------
# 3. PIPELINE: Bi-Directional Reconciliation
# -----------------------------------------------------------------------------

def run_master_audit():
    """Main pipeline for the bi-directional NIAP reconciliation audit.

    Why: To identify gaps between governance tickets and registered products,
    providing forensic insights into approved launches and potential shadow IT.

    Steps:
        1. Extract data from various sources (Jira, Register, Core).
        2. Standardize and normalize data keys.
        3. Perform a bi-directional outer join to identify discrepancies.
        4. Attach technical footprints to identified gaps.
        5. Generate a comprehensive forensic report and supporting data files.
        6. Upload all artifacts to Google Drive.
    """
    try:
        logging.info("PHASE A: Extracting Source Data...")
        with zeus() as cur:
            df_jira = execute_sql(cur, "SELECT issue_id, summary, issue_status, product_register_link FROM global_entity_operations.jira_niap_initiatives", None)
            df_reg  = execute_sql(cur, "SELECT name, niap, product_status FROM global_entity_operations.niap_product_register", None)
            df_core = execute_sql(cur, "SELECT product_type, name, COUNT(*) as risk_volume FROM core.products WHERE (decommission_date IS NULL OR decommission_date > CURRENT_DATE) GROUP BY 1, 2", None)
            df_comp = execute_sql(cur, "SELECT name, business_impact_assessment_importance_score as bia FROM global_entity_operations.sdm_service_catalogue_v3_process_component WHERE type = 'PRODUCT'", None)

        # Standardize Statuses
        df_reg['product_status'] = df_reg['product_status'].fillna('Unknown')
        
        # Create Normalization Keys
        df_jira['join_key'] = df_jira['summary'].apply(normalize_name)
        df_reg['join_key'] = df_reg['name'].apply(normalize_name)

        # --- PHASE 1: CREATE MASTER RECONCILIATION SPINE (OUTER JOIN) ---
        logging.info("Building Master Reconciliation Spine...")
        df_spine = pd.merge(
            df_jira, 
            df_reg, 
            on='join_key', 
            how='outer', 
            suffixes=('_jira', '_reg')
        )

        def categorize_overlap(row):
            """Categorizes the reconciliation outcome for a given row in the spine.

            Args:
                row (pd.Series): A row from the merged DataFrame.

            Returns:
                str: The reconciliation category.
            """
            if pd.notna(row['issue_id']) and pd.notna(row['name']): return "MATCHED"
            if pd.notna(row['issue_id']): return "JIRA_ONLY (Unmapped Ticket)"
            return "REGISTER_ONLY (Unmapped Product)"
        
        df_spine['reconciliation_outcome'] = df_spine.apply(categorize_overlap, axis=1)

        # --- PHASE 2: ATTACH FOOTPRINTS ---
        logging.info("Attaching Footprints...")
        df_core['clean_family'] = df_core['name'].apply(clean_tech_family)
        core_lookup = df_core.groupby('clean_family')['risk_volume'].sum().to_dict()
        df_spine['associated_tech_vol'] = df_spine['join_key'].map(core_lookup).fillna(0)

        # --- PHASE 3: METRICS ---
        venn_metrics = df_spine['reconciliation_outcome'].value_counts()
        jira_gaps = df_spine[
            (df_spine['reconciliation_outcome'] == "JIRA_ONLY (Unmapped Ticket)") & 
            (df_spine['issue_status'].isin(['Approved for Launch', 'Monitoring', 'Development']))
        ][['issue_id', 'summary', 'issue_status']].head(50)

        # --- PHASE 4: GENERATE COMPLETE TABLE OF CONTENTS ---
        # NOW INCLUDES ALL 7 EXPORTED FILES
        toc_data = [
            {"What": "The Manifest (This file).", "Why": "Inventory of all files delivered in this export."},
            {"What": "Master many-to-many outer join between NIAP Jira and the Product Register.", "Why": "The primary investigation tool for Afonso to find unmapped tickets and products."},
            {"What": "Deep-dive forensic report with Venn metrics and gap analysis.", "Why": "Stakeholder-ready summary for NotebookLM and PO outreach planning."},
            {"What": "Register-centric delta file.", "Why": "Used to identify products that exist legally but have no governance oversight."},
            {"What": "List of technical families in Core with no NIAP mapping.", "Why": "Identifies high-volume technical 'Shadow IT' risks."},
            {"What": "Architectural L3 Component mapping to the Register.", "Why": "Identifies drift between architectural design and legal registration."},
            {"What": "Audit execution metadata.", "Why": "Proof of run time and logic version for compliance."}
        ]
        
        df_toc = pd.DataFrame(toc_data)
        df_toc.index = [CSV_TOC, CSV_MASTER_SPINE, REPORT_MD, CSV_BASE_0, CSV_BASE_1_GAPS, CSV_BASE_2, TXT_METADATA]
        df_toc.index.name = "Filename"
        df_toc = df_toc.reset_index()

        # --- 5. REPORT GENERATION ---
        md_report = f"""# NIAP to Product Mapping: Forensic Reconciliation Report

## 1. The Venn Diagram (Master Reconciliation Overview)
*Audit Objective: Understand the overlap between governance (Jira) and registration (GEO Register).*

### 1.1 Global Linkage Summary
{df_to_markdown_manual(venn_metrics, headers=['Reconciliation Outcome', 'Count'])}

### 1.2 High-Priority Jira Gaps (Afonso's Investigation List)
*Definition: 'Approved for Launch' or 'Active' Jira tickets that are NOT found in the Product Register.*
{df_to_markdown_manual(jira_gaps)}

---

## 2. Governance Health (Jira perspective)
### 2.1 Jira Ticket Status Breakdown
{df_to_markdown_manual(df_jira['issue_status'].value_counts(), headers=['Status', 'Ticket Count'])}

---

## 3. Product Health (Register perspective)
### 3.1 Unmapped Register Entries (Orphans)
{df_to_markdown_manual(df_spine[df_spine['reconciliation_outcome'] == "REGISTER_ONLY (Unmapped Product)"][['name', 'product_status']].head(20))}

### 4. Technical Exposure (Core Gaps)
*Definition: Aggregated technical families with no NIAP link, sorted by volume.*
{df_to_markdown_manual(df_core.groupby('clean_family')['risk_volume'].sum().sort_values(ascending=False).head(20))}
"""
        # --- 6. SAVE & UPLOAD ---
        with open(REPORT_MD, 'w') as f: f.write(md_report)
        with open(TXT_METADATA, 'w') as f: f.write(f"NIAP AUDIT LOG\nTimestamp: {datetime.now()}\nLogic: Bi-Directional Spine")
        
        df_toc.to_csv(CSV_TOC, index=False)
        df_spine.to_csv(CSV_MASTER_SPINE, index=False)
        df_spine[df_spine['reconciliation_outcome'].str.contains("REGISTER")].to_csv(CSV_BASE_0, index=False)
        df_core[~df_core['clean_family'].isin(df_spine['join_key'])].to_csv(CSV_BASE_1_GAPS, index=False)
        df_comp.to_csv(CSV_BASE_2, index=False)

        logging.info("Uploading to Drive...")
        for artifact in [CSV_TOC, CSV_MASTER_SPINE, REPORT_MD, CSV_BASE_0, CSV_BASE_1_GAPS, CSV_BASE_2, TXT_METADATA]:
            mime = "text/csv" if ".csv" in artifact else ("text/markdown" if ".md" in artifact else "text/plain")
            upload_to_drive(artifact, mime)

        logging.info("RECONCILIATION COMPLETE.\nTable of Contents: %s\nMaster Investigation File: %s", CSV_TOC, CSV_MASTER_SPINE)

    except Exception: 
        logging.exception("An error occurred during the master audit execution.")

if __name__ == "__main__":
    # Configure logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    run_master_audit()
