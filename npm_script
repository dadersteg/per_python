"""
# -----------------------------------------------------------------------------
# BLUF (BOTTOM LINE UP FRONT):
# This engine creates a bi-directional "Master Spine" that reconciles every 
# NIAP Governance Ticket against every Product Register entry. It identifies 
# gaps in both directions: tickets without products and products without tickets.
# -----------------------------------------------------------------------------
# PURPOSE: 
# To provide Afonso and POs with a single, filterable "Venn Diagram" table for 
# forensic investigation into "Approved for Launch" gaps and "Shadow IT."
# -----------------------------------------------------------------------------
"""

import pandas as pd
import json
import re
import time
import numpy as np
import traceback
from datetime import datetime
from rapidfuzz import process, fuzz, utils

# -----------------------------------------------------------------------------
# 1. SETUP: Configuration and File Naming
# -----------------------------------------------------------------------------
TARGET_FOLDER_ID = "15xKraT4MhWzBvOjMTJ0x21TBIopqfl4E"
TIMESTAMP = datetime.now().strftime("%Y%m%d_%H%M")

# Core Artifacts
REPORT_MD = f"niap_forensic_deep_dive_{TIMESTAMP}.md"
CSV_TOC = f"export_table_of_contents_{TIMESTAMP}.csv"
CSV_MASTER_SPINE = f"niap_master_spine_reconciliation_{TIMESTAMP}.csv"

# Supporting Bases
CSV_BASE_0 = f"base_0_register_delta_master_{TIMESTAMP}.csv"
CSV_BASE_1_GAPS = f"base_1_core_GAPS_RISK_priority_{TIMESTAMP}.csv"
CSV_BASE_2 = f"base_2_component_mapping_{TIMESTAMP}.csv"

# Audit Trail
TXT_METADATA = f"audit_provenance_metadata_{TIMESTAMP}.txt"

# -----------------------------------------------------------------------------
# 2. TOOLS: Data Processing & Formatting
# -----------------------------------------------------------------------------

def df_to_markdown_manual(df, headers=None):
    """Formats data into clean Markdown tables without f-string backslash errors."""
    if df.empty: return "No data available."
    
    if isinstance(df, pd.Series):
        df = df.reset_index()
        df.columns = headers if headers else ["Category", "Count"]
    
    if isinstance(df.index, pd.Index) and df.index.name:
        df = df.reset_index()

    if headers and len(headers) == len(df.columns):
        df.columns = headers
    
    md_headers = df.columns.tolist()
    header_row = f"| {' | '.join(md_headers)} |"
    sep_row = f"| {' | '.join(['---'] * len(md_headers))} |"
    
    body = []
    for _, row in df.iterrows():
        clean_values = []
        for val in row:
            s_val = str(val).replace('|', '\\|').replace('\n', ' ')
            clean_values.append(s_val)
        row_str = "| " + " | ".join(clean_values) + " |"
        body.append(row_str)
        
    return f"{header_row}\n{sep_row}\n" + "\n".join(body)

def normalize_name(text):
    """Clean standard for cross-source matching."""
    if pd.isna(text): return ""
    text = str(text).lower()
    text = re.split(r'[|·]', text)[-1].strip() 
    return utils.default_process(text)

def clean_tech_family(text):
    """Aggregates technical configurations into clean product families."""
    if pd.isna(text): return ""
    text = str(text).upper()
    text = re.sub(r'_(GB|EUR|US|AU|NZ|SG|JP|RO|FR|PL|ES|IT|IE)_?', ' ', text)
    for p in [r'PT_', r'MF_', r'ACQ_', r'REVX_', r'RV_', r'PRCHS_']: text = re.sub(p, ' ', text)
    for n in ['REPRICING','PLAN','BASE','STD','PREM','METAL','PLUS','OFFER','FREE','UL','PR','ST']: text = text.replace(n, '')
    return " ".join(re.sub(r'[^A-Z0-9\s]', ' ', text).split()).lower()

def upload_to_drive(file_path, mime_type):
    for attempt in range(3):
        try:
            upload_file_to_gdrive(file=file_path, name=file_path, mime_type=mime_type, folder_id=TARGET_FOLDER_ID)
            return
        except Exception: time.sleep(5)

# -----------------------------------------------------------------------------
# 3. PIPELINE: Bi-Directional Reconciliation
# -----------------------------------------------------------------------------

def run_master_audit():
    try:
        print(f"[{datetime.now()}] PHASE A: Extracting Source Data...")
        with zeus() as cur:
            df_jira = execute_sql(cur, "SELECT issue_id, summary, issue_status, product_register_link FROM global_entity_operations.jira_niap_initiatives", None)
            df_reg  = execute_sql(cur, "SELECT name, niap, product_status FROM global_entity_operations.niap_product_register", None)
            df_core = execute_sql(cur, "SELECT product_type, name, COUNT(*) as risk_volume FROM core.products WHERE (decommission_date IS NULL OR decommission_date > CURRENT_DATE) GROUP BY 1, 2", None)
            df_comp = execute_sql(cur, "SELECT name, business_impact_assessment_importance_score as bia FROM global_entity_operations.sdm_service_catalogue_v3_process_component WHERE type = 'PRODUCT'", None)

        # Standardize Statuses
        df_reg['product_status'] = df_reg['product_status'].fillna('Unknown')
        
        # Create Normalization Keys
        df_jira['join_key'] = df_jira['summary'].apply(normalize_name)
        df_reg['join_key'] = df_reg['name'].apply(normalize_name)

        # --- PHASE 1: CREATE MASTER RECONCILIATION SPINE (OUTER JOIN) ---
        print(f"[{datetime.now()}] Building Master Reconciliation Spine...")
        df_spine = pd.merge(
            df_jira, 
            df_reg, 
            on='join_key', 
            how='outer', 
            suffixes=('_jira', '_reg')
        )

        def categorize_overlap(row):
            if pd.notna(row['issue_id']) and pd.notna(row['name']): return "MATCHED"
            if pd.notna(row['issue_id']): return "JIRA_ONLY (Unmapped Ticket)"
            return "REGISTER_ONLY (Unmapped Product)"
        
        df_spine['reconciliation_outcome'] = df_spine.apply(categorize_overlap, axis=1)

        # --- PHASE 2: ATTACH FOOTPRINTS ---
        print(f"[{datetime.now()}] Attaching Footprints...")
        df_core['clean_family'] = df_core['name'].apply(clean_tech_family)
        core_lookup = df_core.groupby('clean_family')['risk_volume'].sum().to_dict()
        df_spine['associated_tech_vol'] = df_spine['join_key'].map(core_lookup).fillna(0)

        # --- PHASE 3: METRICS ---
        venn_metrics = df_spine['reconciliation_outcome'].value_counts()
        jira_gaps = df_spine[
            (df_spine['reconciliation_outcome'] == "JIRA_ONLY (Unmapped Ticket)") & 
            (df_spine['issue_status'].isin(['Approved for Launch', 'Monitoring', 'Development']))
        ][['issue_id', 'summary', 'issue_status']].head(50)

        # --- PHASE 4: GENERATE COMPLETE TABLE OF CONTENTS ---
        # NOW INCLUDES ALL 7 EXPORTED FILES
        toc_data = [
            {"What": "The Manifest (This file).", "Why": "Inventory of all files delivered in this export."},
            {"What": "Master many-to-many outer join between NIAP Jira and the Product Register.", "Why": "The primary investigation tool for Afonso to find unmapped tickets and products."},
            {"What": "Deep-dive forensic report with Venn metrics and gap analysis.", "Why": "Stakeholder-ready summary for NotebookLM and PO outreach planning."},
            {"What": "Register-centric delta file.", "Why": "Used to identify products that exist legally but have no governance oversight."},
            {"What": "List of technical families in Core with no NIAP mapping.", "Why": "Identifies high-volume technical 'Shadow IT' risks."},
            {"What": "Architectural L3 Component mapping to the Register.", "Why": "Identifies drift between architectural design and legal registration."},
            {"What": "Audit execution metadata.", "Why": "Proof of run time and logic version for compliance."}
        ]
        
        df_toc = pd.DataFrame(toc_data)
        df_toc.index = [CSV_TOC, CSV_MASTER_SPINE, REPORT_MD, CSV_BASE_0, CSV_BASE_1_GAPS, CSV_BASE_2, TXT_METADATA]
        df_toc.index.name = "Filename"
        df_toc = df_toc.reset_index()

        # --- 5. REPORT GENERATION ---
        md_report = f"""# NIAP to Product Mapping: Forensic Reconciliation Report

## 1. The Venn Diagram (Master Reconciliation Overview)
*Audit Objective: Understand the overlap between governance (Jira) and registration (GEO Register).*

### 1.1 Global Linkage Summary
{df_to_markdown_manual(venn_metrics, headers=['Reconciliation Outcome', 'Count'])}

### 1.2 High-Priority Jira Gaps (Afonso's Investigation List)
*Definition: 'Approved for Launch' or 'Active' Jira tickets that are NOT found in the Product Register.*
{df_to_markdown_manual(jira_gaps)}

---

## 2. Governance Health (Jira perspective)
### 2.1 Jira Ticket Status Breakdown
{df_to_markdown_manual(df_jira['issue_status'].value_counts(), headers=['Status', 'Ticket Count'])}

---

## 3. Product Health (Register perspective)
### 3.1 Unmapped Register Entries (Orphans)
{df_to_markdown_manual(df_spine[df_spine['reconciliation_outcome'] == "REGISTER_ONLY (Unmapped Product)"][['name', 'product_status']].head(20))}

### 4. Technical Exposure (Core Gaps)
*Definition: Aggregated technical families with no NIAP link, sorted by volume.*
{df_to_markdown_manual(df_core.groupby('clean_family')['risk_volume'].sum().sort_values(ascending=False).head(20))}
"""
        # --- 6. SAVE & UPLOAD ---
        with open(REPORT_MD, 'w') as f: f.write(md_report)
        with open(TXT_METADATA, 'w') as f: f.write(f"NIAP AUDIT LOG\nTimestamp: {datetime.now()}\nLogic: Bi-Directional Spine")
        
        df_toc.to_csv(CSV_TOC, index=False)
        df_spine.to_csv(CSV_MASTER_SPINE, index=False)
        df_spine[df_spine['reconciliation_outcome'].str.contains("REGISTER")].to_csv(CSV_BASE_0, index=False)
        df_core[~df_core['clean_family'].isin(df_spine['join_key'])].to_csv(CSV_BASE_1_GAPS, index=False)
        df_comp.to_csv(CSV_BASE_2, index=False)

        print(f"[{datetime.now()}] Uploading to Drive...")
        for artifact in [CSV_TOC, CSV_MASTER_SPINE, REPORT_MD, CSV_BASE_0, CSV_BASE_1_GAPS, CSV_BASE_2, TXT_METADATA]:
            mime = "text/csv" if ".csv" in artifact else ("text/markdown" if ".md" in artifact else "text/plain")
            upload_to_drive(artifact, mime)

        print(f"\n✅ RECONCILIATION COMPLETE.\nTable of Contents: {CSV_TOC}\nMaster Investigation File: {CSV_MASTER_SPINE}")

    except Exception: 
        traceback.print_exc()

if __name__ == "__main__":
    run_master_audit()
